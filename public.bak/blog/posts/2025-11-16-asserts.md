One fine day, I was working on a project that involved a lot of vector distance computations.

This involves taking two floating-point vectors and checking how far apart they are from each other. While there are plenty of options to compute distance, today's topic is [L2 (a.k.a. Euclidean) distance](https://en.wikipedia.org/wiki/Euclidean_distance). It goes roughly like this:

```python
  # assuming v1 and v2 have the same size:
  def l2_distance(v1, v2):
    accumulator = 0.0
    for i in range(len(v1)):
      accumulator += (v1[i] - v2[i]) ** 2
    return math.sqrt(accumulator)
```


And when I mean a lot of such operations were involved, I mean quite a lot:

The piece of software I was building was supposed to perform about 100M of such vector distance comparisons per second, which means a budget of about 10ns per distance call, all included. With your typical CPU running at 3GHz, that's 30 CPU cycles of budget to run the distance function. And that's not really a hard limit: if we can make the latency twice as low, my boss is twice as happy and I'm twice as likely to keep my job. [1]


At this scale, every single bit of performance you can squeeze out is gladly welcomed.
Some common sense suggested that the bottleneck of that implementation is not memory (transferring data from the RAM sticks to the CPU) but compute (performing the subtraction and squaring operations).
Indeed, we are nowhere near hitting the throughput limitation of the RAM and our access patterns are quite predictable (hence cache-friendly, hence latency-friendly). So ideally, we'd like to shift some of the load from the compute to the memory.

The typical way to achieve that is [SIMD instructions](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data). Instead of loading one float from each array, performing the numerical computation on these two numbers and storing the result, we do a *bunch* of these at once. We load, say, 8 contiguous floats from each vector, subtract every element of the first vector from the other *at once*, square all 8 results *at once* and store them in 8 accumulator values *at once*. This makes the CPU go 8x faster. If your memory can follow (in our case, it can), we've successfully sped up our algorithm by a factor of 8. Wonderful news. I'm not getting fired.

So, let's take a look at the actual piece of code I was working with, shall we?

```rust
// dismissing imports and other stuff

// the requested amount of f32's  an SIMD register should hold at once
const SIMD_LANECOUNT : usize = 8;

// args: &self and other are of type &[f32], i.e. references to
// a contiguous block of floating point numbers.
#[inline]
fn l2_squared(&self, othr: &[f32]) -> f32 {
    // sanity checks
    assert_eq!(self.len(), othr.len());
    assert!(self.len().is_multiple_of(SIMD_LANECOUNT));

    // accumulator: 8x zero values
    let mut intermediate_sum_lanes = Simd::<f32, SIMD_LANECOUNT>::splat(0.0);

    // iterate on 8-sized chunks
    let self_chunks = self.as_chunks::<SIMD_LANECOUNT>().0.iter();
    let othr_chunks = othr.as_chunks::<SIMD_LANECOUNT>().0.iter();

    for (&slice_self, &slice_othr) in self_chunks.zip(othr_chunks) {
        //f32simd_slf and f32simd_oth are both 8-sized chunks of the original vector
        let f32simd_slf = SimdF32::from_array(slice_self);
        let f32simd_oth = SimdF32::from_array(slice_othr);

        // compute element-wise difference in one CPU operation and update accumulators
        let diff = f32simd_slf - f32simd_oth;
        intermediate_sum_lanes += diff * diff;
    }

    // sum the 8 accumulators into 1 final floating point number
    intermediate_sum_lanes.reduce_sum() // 8-to-1 sum
}
```

This piece of code meets the performance targets, it is fairly trivially correct [2], job done. Right?

A small devil on my shoulder says otherwise. Look at these two guys:

```rust
    assert_eq!(self.len(), othr.len());
    assert!(self.len().is_multiple_of(SIMD_LANECOUNT));
```

What good are they, really? It seems quite tempting to remove them. We know from the benchmark runs that our vectors have a perfectly reasonable size. And *much worse*, they introduce extra instructions in our machine code. That's like, at least a CPU cycle or two in our budget of 30. Not nothing!
Let's remove them and benchmark again. We'll observe the cost of these asserts in practice.

```
  Asserts  | QPS   | Latency
  ---------------------------
  Enabled  | 123M  | not much
  Disabled | 100M  | a bit more
```
todo replace with the actual numbers

...

Oops? What's happening there?


[1] Sentence added for dramatic purposes. My [boss](https://people.epfl.ch/anne-marie.kermarrec) is actually a very enjoyable person to be around.

[2] I don't really care about floats not really being commutative wrt. addition and stuff. Here, the SIMD implementation does not perform the final sum of the squares in the same order as a sequential implementation, which leads to potential numerical instability. I don't consider this a correctness failure here.
